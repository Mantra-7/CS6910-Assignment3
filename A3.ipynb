{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# import other libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('aksharantar_sampled/hin/hin_train.csv')\n",
    "test_data = pd.read_csv('aksharantar_sampled/hin/hin_test.csv')\n",
    "valid_data = pd.read_csv('aksharantar_sampled/hin/hin_valid.csv')\n",
    "\n",
    "# rename columns ['input_seq', 'target_seq']\n",
    "train_data.columns = ['input_seq', 'target_seq']\n",
    "test_data.columns = ['input_seq', 'target_seq']\n",
    "valid_data.columns = ['input_seq', 'target_seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_seq</th>\n",
       "      <th>target_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bindhya</td>\n",
       "      <td>बिन्द्या</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kirankant</td>\n",
       "      <td>किरणकांत</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yagyopaveet</td>\n",
       "      <td>यज्ञोपवीत</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ratania</td>\n",
       "      <td>रटानिया</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vaganyache</td>\n",
       "      <td>वागण्याचे</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     input_seq target_seq\n",
       "0      bindhya   बिन्द्या\n",
       "1    kirankant   किरणकांत\n",
       "2  yagyopaveet  यज्ञोपवीत\n",
       "3      ratania    रटानिया\n",
       "4   vaganyache  वागण्याचे"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Lang\n",
    "class Lang:\n",
    "    def __init__(self, wordList):\n",
    "        self.char2index = {}\n",
    "        self.char2count = {}\n",
    "        self.index2char = {0: 'A', 1: 'Z'}\n",
    "        self.n_chars = 2\n",
    "\n",
    "        for word in wordList:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        for char in word:\n",
    "            self.addChar(char)\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.char2index:\n",
    "            self.char2index[char] = self.n_chars\n",
    "            self.char2count[char] = 1\n",
    "            self.index2char[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.char2count[char] += 1\n",
    "\n",
    "    def encode(self, word):\n",
    "        embedded = []\n",
    "        for i in range(len(word)):\n",
    "            embedded.append([self.char2index[word[i]]])\n",
    "        return Variable(torch.LongTensor(embedded))\n",
    "\n",
    "    def one_hot_encode(self, word):\n",
    "        one_hot = torch.zeros(len(word), self.n_chars)\n",
    "        for i in range(len(word)):\n",
    "            one_hot[i][self.char2index[word[i]]] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def decode(self, word):\n",
    "        decoded = ''\n",
    "        for i in range(len(word)):\n",
    "            decoded += self.index2char[word[i]]\n",
    "        return decoded\n",
    "    \n",
    "    def decode_one_hot(self, word):\n",
    "        decoded = ''\n",
    "        for i in range(len(word)):\n",
    "            decoded += self.index2char[word[i].argmax()]\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Lang objects\n",
    "eng = Lang(train_data['input_seq'])\n",
    "hin = Lang(train_data['target_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a seq2seq model using 2 RNNs\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # encoder and decoder\n",
    "        self.encoder = nn.RNN(input_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.RNN(hidden_size, hidden_size, n_layers)\n",
    "\n",
    "        # linear layer to get output\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # encoder\n",
    "        output, hidden = self.encoder(input, hidden)\n",
    "        \n",
    "        # decoder\n",
    "        output, hidden = self.decoder(output, hidden)\n",
    "        \n",
    "        # get output\n",
    "        output = self.linear(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(eng.n_chars, 128, hin.n_chars, 1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train 1 input\n",
    "def train(input_variable, target_variable):\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # initialize hidden layer\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    # get output\n",
    "    output, hidden = model.forward(input_variable, hidden)\n",
    "\n",
    "    print(output)\n",
    "    print(target_variable)\n",
    "    \n",
    "    # calculate loss for only 1 input\n",
    "    loss = criterion(output.squeeze(1), target_variable.squeeze(1))\n",
    "        \n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / len(input_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 5.8422e-02, -3.1012e-02, -1.5374e-02,  1.1672e-01, -3.1614e-02,\n",
      "           6.2741e-02,  9.4655e-02, -8.6708e-02,  1.1314e-01, -1.6554e-02,\n",
      "           1.9975e-02, -2.9220e-02, -3.2056e-02,  3.8242e-02, -1.9582e-01,\n",
      "          -5.0839e-02,  1.4054e-02, -4.0073e-02,  7.3279e-02, -6.1698e-02,\n",
      "           1.2384e-02, -3.4292e-02, -4.4332e-02, -8.5935e-02, -2.3491e-02,\n",
      "           4.6409e-03,  1.1455e-02,  6.5165e-02,  9.0315e-02, -4.7754e-04,\n",
      "           5.6276e-02,  2.0614e-02, -7.6677e-02,  5.3173e-03,  1.7214e-02,\n",
      "           2.4812e-02, -9.5182e-02, -6.4312e-02, -6.1101e-02, -4.0227e-02,\n",
      "           4.6829e-02,  4.3193e-02,  9.2885e-02,  5.7450e-02, -5.5630e-03,\n",
      "          -3.7661e-02,  4.2154e-02, -3.7428e-02, -2.3863e-02, -4.5701e-02,\n",
      "          -2.2759e-02,  8.2863e-02,  4.4256e-02, -2.5558e-02, -7.1612e-02,\n",
      "           8.7065e-02,  3.1592e-02, -8.0684e-02,  3.4176e-02, -7.2115e-02,\n",
      "          -7.8599e-03, -3.5380e-02, -4.5364e-02,  1.5377e-02, -7.1890e-02,\n",
      "           1.2490e-01]],\n",
      "\n",
      "        [[ 7.1333e-02, -5.4777e-02, -8.2646e-02,  7.3911e-02, -1.1606e-03,\n",
      "           7.9485e-02,  9.3750e-02, -1.4145e-01,  9.2713e-02, -1.0813e-01,\n",
      "           6.7476e-03, -5.0234e-02, -5.1125e-02, -1.1600e-02, -1.3021e-01,\n",
      "          -1.6841e-02, -2.8052e-02, -7.5664e-02,  5.2200e-02,  1.8131e-02,\n",
      "          -2.0215e-02, -4.7409e-03, -2.4833e-02, -1.0060e-01, -1.1023e-01,\n",
      "           2.1021e-02,  7.9122e-03,  1.8234e-01,  1.2406e-02,  6.1278e-03,\n",
      "           6.8020e-02, -8.0503e-02, -1.3707e-02,  1.3876e-02, -1.2242e-02,\n",
      "           5.2743e-02, -4.0384e-02, -7.9875e-02, -2.4250e-02, -2.4249e-02,\n",
      "          -4.3352e-02,  7.4245e-02,  1.4594e-01,  5.4658e-02, -6.0476e-02,\n",
      "           1.8094e-02,  1.7103e-02,  5.8702e-03, -7.3384e-02,  3.9160e-02,\n",
      "          -2.8172e-02,  6.4237e-02, -7.3866e-03, -2.8193e-02, -1.0097e-01,\n",
      "          -2.0613e-02, -3.2266e-02, -6.3404e-02,  6.0358e-02, -5.5228e-02,\n",
      "           5.4341e-02, -9.4024e-02, -2.1298e-02,  7.1384e-02, -1.5626e-01,\n",
      "           9.2110e-02]],\n",
      "\n",
      "        [[ 5.9980e-02, -4.6393e-02, -3.2941e-02,  6.1098e-02, -4.3773e-02,\n",
      "           1.3141e-01,  1.2392e-01, -9.5297e-02,  1.0505e-01, -4.6675e-02,\n",
      "          -6.1189e-02, -8.5396e-02, -9.5895e-03, -5.3353e-02, -1.7270e-01,\n",
      "          -7.4675e-02,  9.3398e-03, -6.7342e-02,  1.1766e-01, -5.4475e-02,\n",
      "          -3.7873e-04,  5.3787e-02,  6.7787e-03, -1.2188e-01, -4.1782e-02,\n",
      "          -6.4097e-03,  1.2826e-02,  1.0144e-01,  7.2734e-02,  1.3453e-02,\n",
      "           8.5753e-02, -3.8087e-02, -6.0576e-03, -1.2914e-02,  1.2669e-03,\n",
      "          -8.8491e-03, -1.3971e-02, -7.9540e-02, -3.1301e-02, -5.3037e-02,\n",
      "           1.4297e-02,  5.1284e-02,  1.0764e-01,  7.4904e-02, -3.3986e-02,\n",
      "          -8.6366e-02,  3.3097e-02, -1.6295e-02,  7.9141e-04, -1.4530e-03,\n",
      "           2.5463e-03,  7.9382e-02, -2.0223e-02, -5.7045e-02, -1.2364e-01,\n",
      "          -6.0435e-03,  1.8575e-03, -3.3696e-02,  1.0777e-01, -6.5135e-02,\n",
      "           6.8241e-02, -6.7385e-02, -2.3306e-02,  2.9792e-02, -1.7259e-01,\n",
      "           8.0879e-02]],\n",
      "\n",
      "        [[ 5.8314e-02, -5.4655e-02, -1.2524e-02,  6.5275e-02, -3.4439e-03,\n",
      "           7.1824e-02,  1.2716e-01, -1.5223e-01,  1.1168e-01, -7.9605e-02,\n",
      "          -5.4289e-02, -7.2345e-02, -3.9563e-02, -5.1579e-02, -1.3353e-01,\n",
      "          -4.9584e-02,  5.0815e-03, -9.8529e-02,  1.2145e-01, -5.7995e-02,\n",
      "          -6.5990e-02,  9.5891e-04, -2.7024e-02, -8.7452e-02, -7.1978e-02,\n",
      "          -1.4203e-02, -2.3426e-03,  1.4121e-01,  5.8643e-02,  4.1898e-02,\n",
      "           7.4048e-02, -1.8449e-02,  2.0211e-03,  6.5065e-02, -1.5297e-02,\n",
      "           4.8078e-02, -2.6285e-02, -1.2698e-01,  1.6421e-02, -8.4451e-03,\n",
      "          -7.3945e-04,  1.5915e-01,  6.3014e-02, -1.5952e-02, -3.0681e-02,\n",
      "          -3.4243e-02,  3.4653e-02,  5.5533e-02, -6.1536e-02,  8.5620e-02,\n",
      "           3.5152e-02,  1.0001e-01, -8.1246e-03, -2.2351e-02, -1.4922e-01,\n",
      "          -1.8664e-06,  2.1414e-03, -4.6421e-02,  7.9846e-02, -3.4321e-02,\n",
      "          -4.5825e-03, -3.5444e-02, -2.0704e-02,  3.9082e-02, -1.2468e-01,\n",
      "           1.3508e-01]],\n",
      "\n",
      "        [[ 9.9495e-02, -3.6402e-02, -5.8076e-03,  1.0954e-01, -3.6805e-02,\n",
      "           6.1119e-02,  6.2595e-02, -1.8357e-01,  2.5717e-02, -1.0676e-01,\n",
      "          -4.2529e-02, -7.1696e-02,  2.8431e-03, -7.6939e-02, -1.0937e-01,\n",
      "          -4.9222e-02, -1.6165e-02, -1.1216e-01,  6.5105e-02, -5.2227e-02,\n",
      "          -4.4567e-02, -1.2027e-02, -1.2899e-02, -8.1658e-02, -1.4542e-02,\n",
      "          -4.3903e-02, -7.0696e-02,  1.0786e-01,  4.6981e-02,  3.5870e-02,\n",
      "           5.6579e-02, -7.3335e-02, -2.5032e-02,  3.2942e-03, -3.0569e-02,\n",
      "          -4.1492e-02, -5.5644e-02, -9.3332e-02,  3.1636e-02, -1.4063e-02,\n",
      "          -1.3230e-02,  9.4035e-02,  1.0454e-01, -4.0085e-02,  4.7413e-03,\n",
      "          -3.7008e-02,  3.5592e-02,  1.0524e-02, -8.1922e-02,  1.8242e-02,\n",
      "           4.0561e-02,  7.0339e-02, -5.5904e-03, -4.7893e-02, -7.4629e-02,\n",
      "           7.8153e-03,  2.1457e-02, -3.3071e-02,  6.9145e-02,  1.9020e-02,\n",
      "           6.6265e-02, -8.9283e-02,  7.9090e-03,  1.2550e-01, -1.5418e-01,\n",
      "           1.4610e-01]],\n",
      "\n",
      "        [[ 8.6848e-02,  1.4602e-02, -5.7990e-02,  1.1825e-01,  2.3448e-02,\n",
      "           6.1569e-02,  1.3598e-01, -1.6413e-01,  6.7595e-02, -6.3050e-02,\n",
      "          -1.0735e-02, -7.0157e-02, -4.8688e-02, -3.8729e-02, -1.0671e-01,\n",
      "          -8.2181e-02,  4.4199e-02, -7.8444e-02,  6.0423e-02,  7.0093e-03,\n",
      "          -4.5801e-02, -6.2517e-03, -9.9713e-03, -1.1004e-01, -1.8655e-02,\n",
      "           8.2286e-03, -4.6675e-03,  1.2225e-01,  1.0263e-02,  8.8552e-03,\n",
      "           8.4853e-02, -5.1889e-02,  2.6172e-02,  5.0853e-02, -1.9288e-02,\n",
      "           5.7613e-02, -7.6158e-02, -8.3053e-02,  7.0266e-02, -7.0921e-02,\n",
      "          -8.7919e-03,  1.1460e-01,  9.8870e-02, -4.2498e-05, -1.4866e-02,\n",
      "          -2.7416e-02,  9.7879e-02,  3.6114e-02, -9.6781e-02, -1.4822e-02,\n",
      "           1.1231e-01,  6.6068e-02,  1.9548e-02,  1.6606e-02, -8.6178e-02,\n",
      "          -3.1707e-02,  3.6092e-02, -4.8138e-02,  1.3317e-01, -3.1366e-02,\n",
      "           2.3220e-02, -7.4867e-02, -4.2110e-03,  3.1373e-02, -1.3420e-01,\n",
      "           7.8061e-02]],\n",
      "\n",
      "        [[ 8.3775e-02, -7.5219e-02, -8.7836e-02,  6.8142e-02,  3.2054e-02,\n",
      "           1.2780e-01,  1.1264e-01, -8.0026e-02,  6.2081e-02, -8.8219e-02,\n",
      "          -1.2675e-02, -4.3859e-02, -2.6136e-02, -8.5184e-02, -1.2488e-01,\n",
      "          -8.6436e-02, -3.3242e-02, -1.1890e-01,  7.9859e-02, -4.7234e-03,\n",
      "          -4.4981e-02,  7.9077e-02,  2.5509e-02, -1.0189e-01, -6.4399e-02,\n",
      "          -2.8377e-02, -1.2439e-03,  1.4340e-01,  8.8851e-02,  1.5360e-02,\n",
      "           9.7200e-02, -4.5155e-02, -5.0023e-02,  4.4532e-02,  8.4956e-03,\n",
      "           8.2143e-02, -7.0598e-02, -7.9122e-02, -9.3078e-02, -5.6640e-02,\n",
      "          -1.0473e-02,  1.3646e-01,  1.1526e-01,  9.7475e-02, -3.1724e-02,\n",
      "          -7.6350e-02,  8.0442e-02, -6.1838e-03, -6.3836e-02,  4.2389e-02,\n",
      "           3.9157e-02,  1.4491e-01, -4.7280e-02,  1.2675e-02, -8.5286e-02,\n",
      "           1.8126e-02,  1.2016e-02, -1.6020e-02,  8.1195e-02, -4.5136e-02,\n",
      "           4.8070e-02, -2.8751e-02, -8.7711e-02,  8.9442e-02, -1.0215e-01,\n",
      "           1.4804e-01]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (7) to match target batch_size (8).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mantra7/Desktop/Mantra/Sem6/CS6910/CS6910-Assignment3/A3.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mantra7/Desktop/Mantra/Sem6/CS6910/CS6910-Assignment3/A3.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m Variable(\u001b[39minput\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mantra7/Desktop/Mantra/Sem6/CS6910/CS6910-Assignment3/A3.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# input = input.view(len(input), 1, -1)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mantra7/Desktop/Mantra/Sem6/CS6910/CS6910-Assignment3/A3.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train(\u001b[39minput\u001b[39;49m, target)\n",
      "\u001b[1;32m/home/mantra7/Desktop/Mantra/Sem6/CS6910/CS6910-Assignment3/A3.ipynb Cell 8\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, target_variable)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mantra7/Desktop/Mantra/Sem6/CS6910/CS6910-Assignment3/A3.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(target_variable)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mantra7/Desktop/Mantra/Sem6/CS6910/CS6910-Assignment3/A3.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# calculate loss for only 1 input\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mantra7/Desktop/Mantra/Sem6/CS6910/CS6910-Assignment3/A3.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39;49msqueeze(\u001b[39m1\u001b[39;49m), target_variable\u001b[39m.\u001b[39;49msqueeze(\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mantra7/Desktop/Mantra/Sem6/CS6910/CS6910-Assignment3/A3.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# backpropagate\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mantra7/Desktop/Mantra/Sem6/CS6910/CS6910-Assignment3/A3.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (7) to match target batch_size (8)."
     ]
    }
   ],
   "source": [
    "input = eng.one_hot_encode(train_data['input_seq'][0])\n",
    "target = hin.one_hot_encode(train_data['target_seq'][0])\n",
    "\n",
    "input = Variable(input).unsqueeze(1)\n",
    "# input = input.view(len(input), 1, -1)\n",
    "\n",
    "train(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hid = model.forward(eng.one_hot_encode('hello'), model.init_hidden(1))\n",
    "\n",
    "hin.decode_one_hot(out.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
