{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# import other libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Lang\n",
    "class Lang:\n",
    "\tdef __init__(self, wordList):\n",
    "\t\tself.char2index = {'A': 0, 'Z': 1}\n",
    "\t\tself.char2count = {}\n",
    "\t\tself.index2char = {0: 'A', 1: 'Z'}\n",
    "\t\tself.n_chars = 2\n",
    "\n",
    "\t\tfor word in wordList:\n",
    "\t\t\tself.addWord(word)\n",
    "\n",
    "\tdef addWord(self, word):\n",
    "\t\tfor char in word:\n",
    "\t\t\tself.addChar(char)\n",
    "\n",
    "\tdef addChar(self, char):\n",
    "\t\tif char not in self.char2index:\n",
    "\t\t\tself.char2index[char] = self.n_chars\n",
    "\t\t\tself.char2count[char] = 1\n",
    "\t\t\tself.index2char[self.n_chars] = char\n",
    "\t\t\tself.n_chars += 1\n",
    "\t\telse:\n",
    "\t\t\tself.char2count[char] += 1\n",
    "\n",
    "\tdef encode(self, word):\n",
    "\t\tembedded = []\n",
    "\t\tfor i in range(len(word)):\n",
    "\t\t\tembedded.append([self.char2index[word[i]]])\n",
    "\t\treturn Variable(torch.LongTensor(embedded))\n",
    "\t\n",
    "\tdef one_hot_encode(self, word):\n",
    "\t\tone_hot = torch.zeros(len(word), self.n_chars)\n",
    "\t\tfor i in range(len(word)):\n",
    "\t\t\tone_hot[i][self.char2index[word[i]]] = 1\n",
    "\t\treturn one_hot\n",
    "\t\n",
    "\tdef one_hot_encode_char(self, char):\n",
    "\t\tone_hot = torch.zeros(1, self.n_chars)\n",
    "\t\tone_hot[0][self.char2index[char]] = 1\n",
    "\t\treturn one_hot\n",
    "\t\n",
    "\tdef decode(self, word):\n",
    "\t\tdecoded = ''\n",
    "\t\tfor i in range(len(word)):\n",
    "\t\t\tdecoded += self.index2char[word[i]]\n",
    "\t\treturn decoded\n",
    "\t\n",
    "\tdef decode_one_hot(self, word):\n",
    "\t\tdecoded = ''\n",
    "\t\tfor i in range(len(word)):\n",
    "\t\t\tdecoded += self.index2char[word[i].argmax().item()]\n",
    "\t\treturn decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, word):\n",
    "    return [lang.char2index[char] for char in word]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(1)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair, inp_lang, out_lang):\n",
    "    input_tensor = tensorFromSentence(inp_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(out_lang, pair[1])\n",
    "    return (input_tensor.unsqueeze(1), target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "class AksharantarDataset(Dataset):\n",
    "\tdef __init__(self, data, inp_lang, out_lang):\n",
    "\t\tself.data = data\n",
    "\t\tself.inp_lang = inp_lang\n",
    "\t\tself.out_lang = out_lang\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tif torch.is_tensor(idx):\n",
    "\t\t\tidx = idx.tolist()\n",
    "\n",
    "\t\tinp_seq = self.inp_lang.one_hot_encode(self.data['input_seq'][idx]).unsqueeze(1)\n",
    "\t\tout_seq = self.out_lang.one_hot_encode(self.data['target_seq'][idx]).unsqueeze(1)\n",
    "\n",
    "\t\tsample = {'input_seq': inp_seq, 'target_seq': out_seq}\n",
    "\t\treturn sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "\tdef __init__(self, input_size, hidden_size):\n",
    "\t\tsuper(EncoderRNN, self).__init__()\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\n",
    "\t\tself.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\t\tself.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "\tdef forward(self, input, hidden):\n",
    "\t\tembedded = self.embedding(input)\n",
    "\t\toutput = embedded\n",
    "\t\toutput, hidden = self.gru(output, hidden)\n",
    "\t\treturn output, hidden\n",
    "\n",
    "\tdef initHidden(self):\n",
    "\t\treturn torch.zeros(1, 1, self.hidden_size)\n",
    "\t\n",
    "class DecoderRNN(nn.Module):\n",
    "\tdef __init__(self, hidden_size, output_size):\n",
    "\t\tsuper(DecoderRNN, self).__init__()\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\n",
    "\t\tself.embedding = nn.Embedding(output_size, hidden_size)\n",
    "\t\tself.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\t\tself.out = nn.Linear(hidden_size, output_size)\n",
    "\t\tself.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\tdef forward(self, input, hidden):\n",
    "\t\toutput = self.embedding(input).view(1, 1, -1)\n",
    "\t\toutput = F.relu(output)\n",
    "\t\toutput, hidden = self.gru(output, hidden)\n",
    "\t\toutput = self.softmax(self.out(output[0]))\n",
    "\t\treturn output, hidden\n",
    "\n",
    "\tdef initHidden(self):\n",
    "\t\treturn torch.zeros(1, 1, self.hidden_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'hin'\n",
    "train_data = pd.read_csv(f'aksharantar_sampled/{lang}/{lang}_train.csv')\n",
    "test_data = pd.read_csv(f'aksharantar_sampled/{lang}/{lang}_test.csv')\n",
    "valid_data = pd.read_csv(f'aksharantar_sampled/{lang}/{lang}_valid.csv')\n",
    "\n",
    "train_data.columns = ['input_seq', 'target_seq']\n",
    "test_data.columns = ['input_seq', 'target_seq']\n",
    "valid_data.columns = ['input_seq', 'target_seq']\n",
    "\n",
    "inp_lang = Lang(train_data['input_seq'])\n",
    "out_lang = Lang(train_data['target_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(train_data['input_seq'][i], train_data['target_seq'][i]) for i in range(len(train_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\tdef __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "\t\tsuper(Seq2Seq, self).__init__()\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.output_size = output_size\n",
    "\t\tself.n_layers = n_layers\n",
    "\n",
    "\t\t# encoder and decoder\n",
    "\t\tself.encoder = EncoderRNN(input_size, hidden_size)\n",
    "\t\tself.decoder = DecoderRNN(hidden_size, output_size)\n",
    "\n",
    "\tdef train(self, input_tensor, target_tensor, encoder_optimizer, decoder_optimizer, criterion, max_length=50):\n",
    "\t\tencoder_hidden = self.encoder.initHidden()\n",
    "\n",
    "\t\tencoder_optimizer.zero_grad()\n",
    "\t\tdecoder_optimizer.zero_grad()\n",
    "\n",
    "\t\tinput_length = input_tensor.size(0)\n",
    "\t\ttarget_length = target_tensor.size(0)\n",
    "\n",
    "\t\tencoder_outputs = torch.zeros(max_length, self.encoder.hidden_size)\n",
    "\n",
    "\t\tloss = 0\n",
    "\n",
    "\t\tfor ei in range(input_length):\n",
    "\t\t\tencoder_output, encoder_hidden = self.encoder(\n",
    "\t\t\t\tinput_tensor[ei], encoder_hidden)\n",
    "\t\t\tencoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "\t\tdecoder_input = torch.tensor([[0]])\n",
    "\n",
    "\t\tdecoder_hidden = encoder_hidden\n",
    "\t\tfor di in range(target_length):\n",
    "\t\t\tdecoder_output, decoder_hidden = self.decoder(\n",
    "\t\t\t\tdecoder_input, decoder_hidden)\n",
    "\t\t\ttopv, topi = decoder_output.topk(1)\n",
    "\t\t\tdecoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "\t\t\tloss += criterion(decoder_output, target_tensor[di])\n",
    "\t\t\tif decoder_input.item() == 1:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\tencoder_optimizer.step()\n",
    "\t\tdecoder_optimizer.step()\n",
    "\n",
    "\t\treturn loss.item() / target_length\n",
    "\n",
    "\tdef trainIters(self,n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "\t\tstart = time.time()\n",
    "\t\tplot_losses = []\n",
    "\t\tprint_loss_total = 0  # Reset every print_every\n",
    "\t\tplot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "\t\tencoder_optimizer = optim.SGD(self.encoder.parameters(), lr=learning_rate)\n",
    "\t\tdecoder_optimizer = optim.SGD(self.decoder.parameters(), lr=learning_rate)\n",
    "\t\ttraining_pairs = [tensorsFromPair(pairs[i], inp_lang, out_lang)\n",
    "\t\t\t\t\t\tfor i in range(n_iters)]\n",
    "\t\tcriterion = nn.NLLLoss()\n",
    "\n",
    "\t\tfor iter in range(1, n_iters + 1):\n",
    "\t\t\ttraining_pair = training_pairs[iter - 1]\n",
    "\t\t\tinput_tensor = training_pair[0]\n",
    "\t\t\ttarget_tensor = training_pair[1]\n",
    "\n",
    "\t\t\tloss = self.train(input_tensor, target_tensor, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\t\t\tprint_loss_total += loss\n",
    "\t\t\tplot_loss_total += loss\n",
    "\n",
    "\t\t\tif iter % print_every == 0:\n",
    "\t\t\t\tprint_loss_avg = print_loss_total / print_every\n",
    "\t\t\t\tprint_loss_total = 0\n",
    "\t\t\t\tprint('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "\t\t\t\t\t\t\t\t\t\t\titer, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "\t\t\tif iter % plot_every == 0:\n",
    "\t\t\t\tplot_loss_avg = plot_loss_total / plot_every\n",
    "\t\t\t\tplot_losses.append(plot_loss_avg)\n",
    "\t\t\t\tplot_loss_total = 0\n",
    "\n",
    "\tdef predict(self, word, max_length = 20):\n",
    "\t\tinput_tensor = inp_lang.encode(word).unsqueeze(1)\n",
    "\t\tencoder_hidden = self.encoder.initHidden()\n",
    "\n",
    "\t\tinput_length = input_tensor.size(0)\n",
    "\n",
    "\t\tencoder_outputs = torch.zeros(max_length, self.encoder.hidden_size)\n",
    "\n",
    "\t\tfor ei in range(input_length):\n",
    "\t\t\tencoder_output, encoder_hidden = self.encoder(\n",
    "\t\t\t\tinput_tensor[ei], encoder_hidden)\n",
    "\t\t\tencoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "\t\tdecoder_input = torch.tensor([[0]])\n",
    "\n",
    "\t\tdecoded_word = \"\"\n",
    "\n",
    "\t\tdecoder_hidden = encoder_hidden\n",
    "\t\tfor di in range(max_length):\n",
    "\t\t\tdecoder_output, decoder_hidden = self.decoder(\n",
    "\t\t\t\tdecoder_input, decoder_hidden)\n",
    "\t\t\ttopv, topi = decoder_output.data.topk(1)\n",
    "\t\t\tif topi.item() == 1:\n",
    "\t\t\t\tdecoded_word+= ('Z')\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tdecoded_word+= (out_lang.index2char[topi.item()])\n",
    "\n",
    "\t\t\tdecoder_input = topi.squeeze().detach()\n",
    "\n",
    "\t\treturn decoded_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bindhya', 'बिन्द्या'), ('kirankant', 'किरणकांत'), ('yagyopaveet', 'यज्ञोपवीत'), ('ratania', 'रटानिया')]\n"
     ]
    }
   ],
   "source": [
    "print(pairs[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 2s (- 3m 45s) (100 1%) 4.2205\n",
      "0m 4s (- 3m 24s) (200 2%) 4.0561\n",
      "0m 5s (- 3m 12s) (300 3%) 3.3816\n",
      "0m 7s (- 3m 10s) (400 4%) 3.9751\n",
      "0m 9s (- 3m 4s) (500 5%) 3.2993\n",
      "0m 11s (- 2m 58s) (600 6%) 2.8330\n",
      "0m 13s (- 2m 54s) (700 7%) 2.8260\n",
      "0m 15s (- 2m 53s) (800 8%) 3.0585\n",
      "0m 17s (- 2m 54s) (900 9%) 3.2794\n",
      "0m 19s (- 2m 51s) (1000 10%) 3.0329\n",
      "0m 20s (- 2m 47s) (1100 11%) 3.0625\n",
      "0m 22s (- 2m 45s) (1200 12%) 2.9987\n",
      "0m 24s (- 2m 43s) (1300 13%) 3.0309\n",
      "0m 26s (- 2m 42s) (1400 14%) 3.0220\n",
      "0m 28s (- 2m 40s) (1500 15%) 3.0687\n",
      "0m 30s (- 2m 41s) (1600 16%) 3.0023\n",
      "0m 33s (- 2m 41s) (1700 17%) 3.0391\n",
      "0m 35s (- 2m 40s) (1800 18%) 2.9802\n",
      "0m 37s (- 2m 38s) (1900 19%) 2.8965\n",
      "0m 38s (- 2m 35s) (2000 20%) 3.0440\n",
      "0m 40s (- 2m 33s) (2100 21%) 2.8499\n",
      "0m 42s (- 2m 30s) (2200 22%) 2.8577\n",
      "0m 43s (- 2m 27s) (2300 23%) 2.9826\n",
      "0m 45s (- 2m 24s) (2400 24%) 2.9859\n",
      "0m 47s (- 2m 22s) (2500 25%) 2.9463\n",
      "0m 49s (- 2m 20s) (2600 26%) 3.1061\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19836/1775145353.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_chars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_lang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_chars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainIters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19836/3671590539.py\u001b[0m in \u001b[0;36mtrainIters\u001b[1;34m(self, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[0;32m     66\u001b[0m                         \u001b[0mtarget_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_pair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m                         \u001b[0mprint_loss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                         \u001b[0mplot_loss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19836/3671590539.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_tensor, target_tensor, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0mdecoder_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \t\t\tdecoder_output, decoder_hidden = self.decoder(\n\u001b[0m\u001b[0;32m     36\u001b[0m \t\t\t\tdecoder_input, decoder_hidden)\n\u001b[0;32m     37\u001b[0m                         \u001b[0mtopv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19836/4162871142.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hidden)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    161\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2208\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(inp_lang.n_chars, 256, out_lang.n_chars)\n",
    "\n",
    "model.trainIters(10000, learning_rate=0.0001, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'सा््ा<EOS>'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a seq2seq model using 2 RNNs\n",
    "class Seq2Seq(nn.Module):\n",
    "\tdef __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "\t\tsuper(Seq2Seq, self).__init__()\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.output_size = output_size\n",
    "\t\tself.n_layers = n_layers\n",
    "\n",
    "\t\t# encoder and decoder\n",
    "\t\tself.encoder = nn.RNN(input_size, hidden_size, n_layers)\n",
    "\t\tself.decoder = nn.RNN(hidden_size, hidden_size, n_layers)\n",
    "\n",
    "\t\t# linear layer to get output\n",
    "\t\tself.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\tdef forward(self, input, hidden):\n",
    "\t\t# encoder\n",
    "\t\toutput, hidden = self.encoder(input, hidden)\n",
    "\t\t\n",
    "\t\t# decoder\n",
    "\t\toutput, hidden = self.decoder(output, hidden)\n",
    "\t\t\n",
    "\t\t# get output\n",
    "\t\toutput = self.linear(output)\n",
    "\t\treturn output, hidden\n",
    "\t\n",
    "\tdef predict(self, input, inp_lang, out_lang):\n",
    "\t\tout, hidden = self.forward(inp_lang.one_hot_encode(input).unsqueeze(1), self.init_hidden(1))\n",
    "\t\treturn out_lang.decode_one_hot(out)\n",
    "\t\n",
    "\tdef init_hidden(self, batch_size):\n",
    "\t\treturn Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator:\n",
    "\tdef __init__(self, lang):\n",
    "\t\ttrain_data = pd.read_csv(f'aksharantar_sampled/{lang}/{lang}_train.csv')\n",
    "\t\ttest_data = pd.read_csv(f'aksharantar_sampled/{lang}/{lang}_test.csv')\n",
    "\t\tvalid_data = pd.read_csv(f'aksharantar_sampled/{lang}/{lang}_valid.csv')\n",
    "\n",
    "\t\ttrain_data.columns = ['input_seq', 'target_seq']\n",
    "\t\ttest_data.columns = ['input_seq', 'target_seq']\n",
    "\t\tvalid_data.columns = ['input_seq', 'target_seq']\n",
    "\n",
    "\t\tself.inp_lang = Lang(train_data['input_seq'])\n",
    "\t\tself.out_lang = Lang(train_data['target_seq'])\n",
    "\n",
    "\t\tself.model = Seq2Seq(self.inp_lang.n_chars, 10, self.out_lang.n_chars, 1)\n",
    "\t\tself.criterion = nn.CrossEntropyLoss()\n",
    "\t\tself.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "\t\ttrain_dataset = AksharantarDataset(train_data, self.inp_lang, self.out_lang)\n",
    "\t\ttest_dataset = AksharantarDataset(test_data, self.inp_lang, self.out_lang)\n",
    "\t\tvalid_dataset = AksharantarDataset(valid_data, self.inp_lang, self.out_lang)\n",
    "\n",
    "\t\tself.train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\t\tself.test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\t\tself.valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\tdef translate(self, word):\n",
    "\t\treturn self.model.predict(word, self.inp_lang, self.out_lang)\n",
    "\t\n",
    "\tdef train_one(self, inp, target):\n",
    "\t\t# zero gradients\n",
    "\t\tself.optimizer.zero_grad()\n",
    "\t\t\n",
    "\t\t# initialize hidden layer\n",
    "\t\thidden = self.model.init_hidden(1)\n",
    "\t\t\n",
    "\t\t# get output\n",
    "\t\toutput, hidden = self.model.forward(inp, hidden)\n",
    "\t\t\n",
    "\t\tmx_len = min(len(output), len(target))\n",
    "\n",
    "\t\t# append output and target with 'Z' to make them of mx_len\n",
    "\t\twhile(len(output) < mx_len):\n",
    "\t\t\toutput = torch.cat((output, self.out_lang.one_hot_encode_char('Z')), 0)\n",
    "\n",
    "\t\twhile(len(target) < mx_len):\n",
    "\t\t\ttarget = torch.cat((target, self.out_lang.one_hot_encode_char('Z')), 0)\n",
    "\t\t\n",
    "\t\t# calculate loss \n",
    "\t\tloss = self.criterion(torch.flatten(output[:mx_len], 0, 1), torch.flatten(target[:mx_len], 0, 1).max(1)[1])\n",
    "\t\t\t\n",
    "\t\t# backpropagate\n",
    "\t\tloss.backward()\n",
    "\t\t\n",
    "\t\t# update weights\n",
    "\t\tself.optimizer.step()\n",
    "\n",
    "\t\treturn loss.data.item() / len(inp)\n",
    "\t\n",
    "\tdef train_epoch(self, data_loader):\n",
    "\t\tloss = 0\n",
    "\t\tfor i_batch, sample_batched in tqdm(enumerate(data_loader)):\n",
    "\t\t\tloss += self.train_one(sample_batched['input_seq'][0], sample_batched['target_seq'][0])\n",
    "\t\tprint(' Loss: ', loss / len(data_loader))\n",
    "\t\treturn loss / len(data_loader)\n",
    "\t\n",
    "\tdef train(self, epochs):\n",
    "\t\tlosses = []\n",
    "\t\tfor epoch in range(epochs):\n",
    "\t\t\tprint('Epoch ', epoch + 1)\n",
    "\t\t\tloss = self.train_epoch(self.train_dataloader)\n",
    "\t\t\tlosses.append(loss)\n",
    "\t\treturn losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51199it [07:55, 107.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss:  0.2866536472512427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2866536472512427]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi_trans = Translator('urd')\n",
    "hindi_trans.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ساریااااا'"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi_trans.translate('saptarshi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(hindi_trans, open('urd_trans', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51199it [03:09, 270.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss:  0.2978701331155101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2978701331155101]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guj_trans = Translator('guj')\n",
    "guj_trans.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ફ'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guj_trans.translate('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(guj_trans, open('guj_trans', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51199it [05:19, 160.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss:  0.18516780389609597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18516780389609597]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kan_trans = Translator('kan')\n",
    "kan_trans.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ಅಭ್ದಲಲಳಿ'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kan_trans.translate('abdullah')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
